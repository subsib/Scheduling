\section{Objectif des tests}

	Dans cette partie, nous allons exposer quels tests nous avons effectué ainsi que 
	la méthodologie utilisée.
	Nous avons plusieurs attentes que vous souhaitons évaluer avec des tests. \newline
	 
	Tout d'abord, dans le processus de réalisation de l'algorithme, nous devons nous assurer 
	que notre implémentation est correcte. Nous souhaitons également vérifier par des tests 
	sur du matériel dédié les hypothèses émises \todo{inclure ref} plus tôt dans ce travail, 
	à savoir : vérifier que les surcoûts augment avec le nombre de cœurs, de tâches, 
	de périodes différentes. 
	Nous chercherons à confronter des résultats théoriques avec une réalisation.
	Nous pouvons également comparer la charge des processeurs avec Global-EDF pour des mêmes ensembles de tâches.

%	Nous voulons :
%	\begin{enumerate}
%	\setlength\itemsep{0.1em}
%		\item Montrer qu'on a implémenté le bon algorithme
%		\item Regarder les temps de surcoût sur des tests basiques
%		\begin{enumerate}
%			\item mesurer les WCET
%			\item mesurer les temps d'exécution et retirer tout ce qui n'est pas de l'exécution = overhead
%			\item voir si ça augmente bien avec le nombre de cœur, avec plus ou moins de tâches
%		\end{enumerate}
%		\item voir si avec des tâches de périodes harmoniques, on arrive à des trucs sympas
%		\item mesurer le temps d'occupation des procs
%		\item comparer pour tout ça avec Global-EDF
%		\item trouver des trucs faisables avec UEDF, pas faisables avec global, et inversement
%	\end{enumerate}
	

\section{Matériel utilisé}
	\textbf{HIPPEROS} est peut être installé sur un nombre limité de machines. 
	L'une d'elle est une \textbf{SABRE Lite} dont voici les spécifications :
	
	\subsubsection{SABRE Lite i.MX6}
		\begin{enumerate}
			\setlength\itemsep{0.1em}
			\item Architecture \textbf{ARMv7} \textbf{Cortex-A9}
			\item Processeur Freescale i.MX6 Quad 1GHz
			\item 1 GB de ram
			\item Mémoire sur carte micro-SD
		\end{enumerate}
	C'est un kit de développement couramment utilisé. Les spécifications qui 
	ont vraiment de l'importance ici sont le nombre de cœurs et la mémoire ram, 
	le reste étant assez secondaire dans les tests que nous réalisons.

\section{Création de tâche}

	\subsubsection{Description de tâche dans HIPPEROS}
	Pour exécuter des tâches avec \textbf{HIPPEROS}, il faut définir des fichiers de configuration 
	sous un format \textit{XML}, dans ce fichier doivent apparaître certaines propriétés des tâches. 
	Dans notre cas :
	\begin{itemize}
		\setlength\itemsep{0.1em}
		\item le nom
		\item l'adresse de l'exécutable
		\item Si elles sont en temps réel
		\item Si elles ont une ou plusieurs occurrences \{\textit{OneShot; Periodic; Sporadic}\}
		\item leur WCET
		\item leur période
		\item leur échéance
	\end{itemize}
	et ceci autant de fois que de tâches que l'on aura à décrire. \newline
	
	Les tests que nous faisons passer doivent nous permettre de contrôler au mieux la durée réelle 
	d'exécution de la tâche, afin de maîtriser l'écart entre cette valeur et le WCET attribué dans les 
	fichiers de configuration. \newline
	
	Pour ce faire, nous créons un unique programme qui sera lancé autant de fois que de tâches déclarées 
	dans le fichier de configuration. 
	Nous définissons un fichier de \textit{header} contenant un tableau de valeurs. 
	Ainsi, lors de chaque exécution, le programme va lire une valeur dans un tableau, qui représente la 
	limite de temps qu'il devra atteindre avant de se terminer proprement.\newline
	
	Concrètement, un programme \textbf{Measure} possède un \textit{Process Id}, et exécute cette boucle :

	\label{algomeasure}
	\begin{algorithm}[H]
		\caption{Measure\_main}
		\begin{algorithmic}
			\STATE $boundary \leftarrow BoundaryList[Process\_id] + now$
			\REPEAT \item $t = now$
			\UNTIL{ $t < boundary$ }
		\end{algorithmic}
	\end{algorithm}

	Cette tâche périodique est relancée autant de fois que nécessaire; c'est à dire qu'à la fin 
	d'une boucle, elle est suspendue, puis réactivée à chaque période.
	
	\subsubsection{Déroulement d'un test}
	Nous créons un fichier contenant les informations utiles sur le test :
	\begin{itemize}
		\item WCET
		\item Offset
		\item Période
		\item Échéance
	\end{itemize}
	Nous exécutons ensuite les programmes compilés sur la \textit{SABRE Lite}, en nous assurant qu'il n'y a pas 
	de dépassement de WCET ou de période. Lorsque l'ensemble a été validé en test, il sera 
	exécuté plus longuement (1 à 2h) afin d'être analysé ultérieurement.\newline

	Il est important de signaler ici qu'\textbf{HIPPEROS} permet de générer des \textit{logs}.
	En branchant sur port USB une sonde \textbf{Olimex} \todo{add ref}, il est possible de récupérer 
	un certain nombre d'informations sur port série. Toutefois, la production de \textit{logs}
	fausse elle-même les données puisqu'elle prend un certain temps en provoquant des écritures 
	sur le port série.
	Par conséquent, les résultats présentés ici ne peuvent 
	être considérés autrement que comme des approximations.

\section{Détermination des WCET des tâches}
	\textbf{UEDF} est optimal, en \underline{théorie}. Revenons un instant sur ce point. 
	Cela signifie qu'en omettant tous les surcoûts, l'on est capable de charger les processeurs 
	à $100\%$ d'utilisation. \newline

	\todo{ajouter blabla sur scope du WCET et que c'est documenté + liens trouvés}	
	En pratique, le WCET d'une tâche est une valeur difficile à donner, et qui va dépendre de 
	beaucoup de facteurs.\newline
	
	Elle dépend :
	\begin{enumerate}
		\item du nombre d'opérations à faire dans le code (approximation du nombre d'instructions) et de leur type (IO, calcul...)
		\item de la plateforme d'exécution de la tâche (les caractéristiques de la machine)
		\item de l'algorithme 
	\end{enumerate}
	
	\subsection{Nombre et type d'opération}
	Le pire temps d'exécution peut varier en fonction du nombre d'instructions et surtout de leur type. En effet, 
	on peut penser qu'une tâche qui doit uniquement faire du calcul prend un temps déterministe, en pratique, 
	il y aura toujours quelques modifications. \newline
	Par ailleurs, certaines instructions prennent un temps plus ou moins variable. Par exemple, 
	les entrées-sorties ont un temps qui peut fort varier. En outre, selon la disponibilité ou 
	des ressources, des temps peuvent s'ajouter. 
	ne suffisent pas.
	
	\subsection{La plateforme d'exécution}
	Pour des raisons évidentes, une tâche ne mettra pas un temps équivalent sur une plateforme ou sur une autre, 
	et le temps n'est donc pas une propriété théorique de la tâche elle-même mais dépend bien 
	également de la plateforme. On peut bien avoir une idée en comptant le nombre d'instructions et en faisant 
	des calculs par rapport à la fréquence du processeur, mais cela reste à évaluer en pratique.\newline
	
	Dans un cas idéal où le WCET ne dépendrait que de la machine et du nombre d'instructions, 
	on aurait pu imaginer faire un certain nombre d'exécutions de la même tâche et prendre une valeur 
	statistiquement en dehors des valeurs possibles avec certitude de $n\%$. \todo{nommer ce test statistique}
	Toutefois, cette proposition, qui a d'abord été faite, comme on peut le voir sur les figures \todo{add ref} 	

	\subsection{L'algorithme}
	
	En fin de compte, l'algorithme va ici avoir un impact sur le WCET. 
	Sur Global-EDF, le WCET ne fait partie des paramètres utilisés pour calculer 
	l'ordonnancement. La variable déterminante est l'échéance absolue ($d_i(t)$). 
	Le WCET en revanche est déterminant dans le cas d'\textbf{UEDF}. Voyons 
	l'impact de cette variable et les risques liés à l'évaluation de cette valeur.\newline
	
	En bref rappel de la partie \todo{ref explication UEDF}, 
	Le WCET va permettre de calculer l'\textit{utilisation} de la tâche. 
	Cette proportion est utilisée pour déterminer le nombre d'unités de temps d'exécution 
	du travail $i$ attribuées au processeur $\pi_j$. \newline
	En pratique, il y a un temps entre le moment où est calculé l'ordonnancement et le 
	moment où l'exécution commence réellement. Celui-ci est considéré comme du surcoût, et 
	fait partie des éléments que nous cherchons à connaître. \newline
	
	Au moment de commencer les tests, il est donc très difficile d'évaluer le rapport 
	entre les temps d'exécutions réels des tâches et les WCET que nous devons fixer. \newline
	
	Cette procédure d'évaluation a consisté au départ à paramétrer très manuellement les WCET et 
	durées d'exécutions demandées, pour proposer finalement une façon plus systématique 
	de fixer ces valeurs.

		
\subsection{Récolte des résultats}
	Afin de pouvoir analyser les performances de l'implémentation d'\textbf{UEDF}, il nous faut être capable de reconstituer 
	l'historique d'une exécution. \newline
	
	Cela est possible en produisant des sorties (\textit{logs}) que nous parsons à l'aide d'outils 
	développés en \textit{Python}. Cela étant, il nous faut préciser que la production de sorties sur le 
	port série de la \textit{SABRE Lite} prend elle-même du temps. Par conséquent, 
	les résultats avancés ne peuvent être qu'approximatifs, particulièrement les surcoûts.\newline
	
	Pour générer ces logs, nous avons utilisé les outils disponibles d'\textbf{HIPPEROS}, et avons ajouté des informations 
	sur les temps d'exécution de l'ordonnanceur lui-même (temps passé dans les fonctions et nombre d'occurrences).
	

\section{Comparaison avec G-EDF}

	Les expériences réalisées avec \textbf{UEDF }ont toutes été également réalisées avec \textbf{Global EDF}.
	Les mêmes ensembles de tâches ont été rejoués en changeant juste l'ordonnanceur. Pour certains exemples, comme
	on le verra dans la partie suivante, les ensembles ont été un peu modifiés également afin de 
	mettre en lumière des comportements intéressants et inattendus compte tenu de la littérature.